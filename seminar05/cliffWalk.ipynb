{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA and Q-Learning: Cliffworld Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we use the cliffworld example to implement SARSA and Q-Learning,\n",
    "and to illustrate how their behavior differs.\n",
    "\n",
    "For a detailed description of the setting see\n",
    "[Example 6.6 in *Sutton & Barto*](http://incompleteideas.net/book/RLbook2020trimmed.pdf#page=154)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gridworld import GridWorld \n",
    "from collections import defaultdict\n",
    "\n",
    "# Use `tk` for interactive play, `inline` for normal plots:\n",
    "# %matplotlib tk\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we recreate the setting from the example using our generic gridworld class.\n",
    "\n",
    "*(How this is done is somewhat specific to our implementation and the details do not matter too much.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension and start/goal squares\n",
    "HEIGHT = 4\n",
    "WIDTH = 12\n",
    "START = (HEIGHT - 1, 0)\n",
    "GOAL = (HEIGHT - 1, WIDTH - 1)\n",
    "\n",
    "# Create (empty) gridworld instance\n",
    "gw = GridWorld(HEIGHT, WIDTH, START)\n",
    "\n",
    "# Label start/goal\n",
    "gw.positionLabels[START] = 'S'\n",
    "gw.positionLabels[GOAL] = 'G'\n",
    "\n",
    "# Add cliffs to the bottom row (except for corners)\n",
    "for i in range(1, WIDTH - 1):\n",
    "    gw.immediateTeleportations[(HEIGHT - 1, i)] = (START, -200)\n",
    "\n",
    "# Make goal state terminal\n",
    "gw.teleportations[GOAL] = (GOAL, 0)\n",
    "\n",
    "# Give -1 reward for all other transitions\n",
    "gw.rewards[START] = -1\n",
    "gw.invalidActionReward = -1\n",
    "for i in range(HEIGHT - 1):\n",
    "    for j in range(WIDTH):\n",
    "        gw.rewards[(i, j)] = -1\n",
    "\n",
    "# A small chance of doing something random, just to make a point\n",
    "gw.randomChance = 0.0\n",
    "\n",
    "# Helper list, containing all possible states and actions\n",
    "ALL_STATES = list((i, j) for i in range(gw.height) for j in range(gw.width))\n",
    "ACTIONS = [0, 1, 2, 3]\n",
    "\n",
    "# Plot the world\n",
    "plt.figure(figsize=(10,3))\n",
    "gw.drawWorld()\n",
    "plt.show()\n",
    "\n",
    "# # Play in the world\n",
    "# gw.play()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some parameters used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "ALPHA = 0.1\n",
    "\n",
    "# Exploration rate\n",
    "EPSILON = 0.1\n",
    "\n",
    "# Discount factor\n",
    "GAMMA = 1.0\n",
    "\n",
    "# Number of episodes in SARSA/Q-learning\n",
    "N_EPISODES = 10000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we implement SARSA to solve the cliff world environment.\n",
    "\n",
    "We start by implementing a helper function that lets us choose an action in an epsilon-greedy manner, according to our current estimate of the state-action values $q(s,a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseEpsilonGreedy(allQValues, state, eps):\n",
    "    # Choose random action with probability `eps`\n",
    "    # ...\n",
    "    \n",
    "    # Chose optimal action (breaking ties randomly)\n",
    "    # ...\n",
    "    pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement SARSA itself, following the pseudocode in *Sutton & Barto*, page 130."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q(s,a) arbitrarily\n",
    "# Repeat (for each episode):\n",
    "#     Initialize s\n",
    "#     Choose a from s using policy derived from Q (e.g., epsilon-greedy)\n",
    "#     Repeat (for each step of episode):\n",
    "#         Take action a, observe r, s'\n",
    "#         Choose a' from s' using policy derived from Q (e.g., epsilon-greedy)\n",
    "#         Q(s,a) <- Q(s,a) + alpha * [r + gamma * Q(s',a') - Q(s,a)]\n",
    "#         s <- s'; a <- a';\n",
    "#     until s is terminal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsaEpisode(world: GridWorld, qValues):\n",
    "    # ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q(s,a) arbitrarily\n",
    "# ...\n",
    "\n",
    "# Repeat (for each episode):\n",
    "# ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we illustrate the result of SARSA by looking at the paths taken during training, the estimated value function, and the corresponding optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot paths taken at various stages\n",
    "plt.figure(figsize=(12, 3))\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "# ...\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "# ...\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "# ...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the optimal policy and state values\n",
    "policy = dict()\n",
    "values = dict()\n",
    "\n",
    "# Compute policy/value for each state\n",
    "# ...\n",
    "\n",
    "# Plot policy\n",
    "plt.figure(figsize=(10,3))\n",
    "gw.drawWorld(drawRewards=False, policy=policy)\n",
    "plt.show()\n",
    "\n",
    "# Plot values\n",
    "plt.figure(figsize=(10,3))\n",
    "gw.drawWorld(drawRewards=False, values=values)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more basic way to plot the policy\n",
    "labels = ['U', 'R', 'D', 'L']\n",
    "for i in range(gw.height):\n",
    "    for j in range(gw.width):\n",
    "        # ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to play and plot a greedy game (w.r.t. the supplied Q-values)\n",
    "def illustrateGreedyGame(world: GridWorld, qValues):\n",
    "    world.reset()\n",
    "    states = [world.pos]\n",
    "    action = chooseEpsilonGreedy(qValues, world.pos, 0.0)\n",
    "    actions = [action]\n",
    "    rewards = [0]\n",
    "    while world.pos != GOAL:\n",
    "        newPos, reward = world.step(action)\n",
    "        action = chooseEpsilonGreedy(qValues, world.pos, 0.0)\n",
    "        rewards.append(reward)\n",
    "        states.append(world.pos)\n",
    "        actions.append(action)\n",
    "    plt.figure(figsize=(10,3))\n",
    "    gw.drawWorld(drawRewards=False, path=states)\n",
    "    plt.show()\n",
    "    print('Total reward:', sum(rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrateGreedyGame(gw, sarsaQValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how often each state was visited\n",
    "sarsaHitCounts = defaultdict(lambda: 0)\n",
    "# ...\n",
    "\n",
    "# Plot hit counts\n",
    "plt.figure(figsize=(12, 3))\n",
    "# ...\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we implement Q-learning to solve the cliff world environment, \n",
    "following the pseudocode in *Sutton & Barto*, page 131."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q(s,a) arbitrarily\n",
    "# Repeat (for each episode):\n",
    "#     Initialize s\n",
    "#     Repeat (for each step of episode):\n",
    "#         Choose a from s using policy derived from Q (e.g., epsilon-greedy)\n",
    "#         Take action a, observe r, s'\n",
    "#         Q(s,a) <- Q(s,a) + alpha * [r + gamma * max_a' Q(s',a') - Q(s,a)]\n",
    "#         s <- s';\n",
    "#     until s is terminal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qLearningEpisode(world: GridWorld, qValues):\n",
    "    # ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q(s,a) arbitrarily\n",
    "# ...\n",
    "\n",
    "# Repeat (for each episode):\n",
    "# ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate the results of Q-learning similarly to the results of SARSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot paths taken at various stages\n",
    "plt.figure(figsize=(12,3))\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "# ...\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "# ...\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "# ...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the optimal policy and state values\n",
    "policy = dict()\n",
    "values = dict()\n",
    "\n",
    "# Plot the optimal policy and state values\n",
    "# ...\n",
    "\n",
    "# Plot policy\n",
    "plt.figure(figsize=(10,3))\n",
    "gw.drawWorld(drawRewards=False, policy=policy)\n",
    "plt.show()\n",
    "\n",
    "# Plot values\n",
    "plt.figure(figsize=(10,3))\n",
    "gw.drawWorld(drawRewards=False, values=values)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['U', 'R', 'D', 'L']\n",
    "for i in range(gw.height):\n",
    "    for j in range(gw.width):\n",
    "        # ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the illustrate function from above\n",
    "# illustrateGreedyGame(gw, qLearningQValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how often each state was visited\n",
    "qLearningHitCounts = defaultdict(lambda: 0)\n",
    "# ...\n",
    "\n",
    "# Plot hit counts\n",
    "plt.figure(figsize=(12, 3))\n",
    "# ...\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of per-episode rewards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we compare the per-episode rewards of SARSA and Q-learning.\n",
    "Plotting a moving average might be more insightful than raw rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute a moving average\n",
    "def movingAverage(a, k):\n",
    "    return np.convolve(a, np.ones(k), 'valid') / k\n",
    "\n",
    "# Plot (moving average) of per-episode rewards\n",
    "N_AVERAGE = 100\n",
    "plt.figure()\n",
    "# ...\n",
    "plt.show()\n",
    "\n",
    "# Compute average over the second half of episodes\n",
    "print('Long-term average rewards\\n---')\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "275c90bd5bb82664c788af040251692cc03dc86a881c38c70c21622899dbd0c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
